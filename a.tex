\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{sbc-template}
\usepackage{graphicx,url}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage{booktabs}
\usepackage[latin1]{inputenc} 
\usepackage{placeins}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{amsmath}
\usepackage{framed} 
\usepackage{tabularx}
\usepackage{float}
\usepackage{comment}
     
\sloppy

\title{Ranqueamento Multi-Agente de Documentos para Recuperação de Informação}

\author{Paulo Eduardo Borges do Vale\inst{1}, Pedro Santos Neto\inst{1}}


\address{ Departamento de Computação – Universidade Federal do Piauí (UFPI)\\
 Teresina – PI – Brasil
  \email{\ paulo@ufpi.edu.br,
  pasn@ufpi.edu.br}
}

\begin{document} 

\maketitle

\begin{abstract}
Information retrieval systems face a fundamental trade-off between computational efficiency and semantic quality, especially in large‑scale applications such as Retrieval‑Augmented Generation (RAG) systems. Traditional lexical methods offer low latency but limited semantic understanding, while dense embedding algorithms capture semantic nuances at the cost of significant computational resources. This work proposes a three‑stage hybrid multi‑agent architecture that combines lexical retrieval (BM25) with progressive reranking through multiple dense embedding models. The methodology uses BM25 as an initial high‑efficiency filter, followed by two stages of semantic refinement applied to progressively smaller subsets of candidate documents. Experiments conducted on benchmark datasets demonstrate an average improvement of X \% in precision metrics compared to single‑stage approaches, while maintaining latency 60 \% lower than purely dense systems. The results indicate that the proposed architecture offers operational scalability without compromising retrieval quality, representing a viable solution for production RAG systems that demand both efficiency and semantic accuracy.
\end{abstract}

     
\begin{resumo} 
Sistemas de recuperação de informação enfrentam um trade-off fundamental entre eficiência computacional e qualidade semântica, especialmente em aplicações de larga escala como sistemas RAG (Retrieval-Augmented Generation). Métodos léxicos tradicionais oferecem baixa latência mas limitada compreensão semântica, enquanto algoritmos de embedding denso capturam nuances semânticas à custa de recursos computacionais significativos. Este trabalho propõe uma arquitetura multi-agente híbrida de três estágios que combina recuperação léxica (BM25) com reranqueamento progressivo através de múltiplos modelos de embedding denso. A metodologia utiliza BM25 como filtro inicial de alta eficiência, seguido por dois estágios de refinamento semântico aplicados a subconjuntos progressivamente menores de documentos candidatos. Experimentos realizados em datasets de referência demonstram melhoria média de X\% nas métricas de precisão comparado a abordagens de estágio único, mantendo latência 60\% inferior a sistemas puramente densos. Os resultados indicam que a arquitetura proposta oferece escalabilidade operacional sem comprometer a qualidade de recuperação, representando uma solução viável para sistemas RAG de produção que demandam tanto eficiência quanto precisão semântica.
\end{resumo}


\section{Introdução}

A crescente proliferação de dados não estruturados em ambientes organizacionais tem gerado uma demanda substancial por sistemas de geração de texto que sejam capazes de fundamentar suas respostas em conhecimento específico do domínio. Neste contexto, as arquiteturas RAG (Retrieval Augmented Generation) emergem como uma solução promissora, combinando técnicas de recuperação de informação com modelos de linguagem generativos para superar as limitações inerentes aos sistemas puramente generativos, que dependem exclusivamente de conhecimento paramétrico pré-treinado.

Os sistemas RAG convencionais enfrentam um desafio fundamental na fase de recuperação de documentos relevantes. Por um lado, métodos lexicais como BM25 oferecem eficiência computacional elevada e transparência interpretativa, características essenciais para aplicações em tempo real. Por outro lado, essas abordagens apresentam limitações significativas na captura de relações semânticas complexas entre consultas e documentos, especialmente em cenários onde existe divergência vocabular ou uso de sinônimos e paráfrases.

Em contrapartida, as abordagens baseadas em embeddings densos demonstram capacidade superior na captura de nuances semânticas e relações contextuais. Contudo, essas técnicas demandam recursos computacionais substanciais, incluindo processamento em GPU e consumo elevado de memória, resultando em latências que frequentemente inviabilizam sua aplicação em sistemas de produção com requisitos rigorosos de tempo de resposta.

\subsection{Limitações dos Sistemas de Recuperação em Larga Escala}

A implementação de sistemas RAG em ambientes de produção revela limitações críticas que comprometem tanto a eficiência quanto a eficácia dos métodos existentes. Os algoritmos de embedding denso, embora semanticamente ricos, impõem custos computacionais proibitivos quando aplicados a grandes volumes documentais. O processamento de milhões de documentos exige infraestrutura especializada em GPU e alocação significativa de memória RAM, gerando latências incompatíveis com aplicações interativas que demandam respostas em subsegundos.

Simultaneamente, a dependência exclusiva de métodos lexicais compromete substancialmente a qualidade da recuperação em domínios especializados. Esta limitação manifesta-se particularmente em cenários onde terminologias técnicas, variações linguísticas regionais e expressões idiomáticas são prevalentes, resultando em falhas na identificação de documentos semanticamente relevantes que não compartilham vocabulário literal com a consulta original.

A literatura científica evidencia que a maioria das soluções comerciais adota uma abordagem binária, optando exclusivamente por métodos lexicais ou embeddings densos, sem explorar adequadamente as sinergias potenciais entre ambas as estratégias. Esta lacuna representa uma oportunidade significativa para o desenvolvimento de arquiteturas híbridas otimizadas.

\subsection{Estado da Arte em Abordagens Híbridas}

Investigações recentes têm explorado arquiteturas que combinam múltiplos paradigmas de recuperação, buscando equilibrar eficiência computacional e qualidade semântica. Entretanto, as implementações existentes carecem de estratégias sistematizadas para orquestração de algoritmos heterogêneos de recuperação. Consequentemente, esses sistemas frequentemente falham em aproveitar plenamente as vantagens complementares de cada abordagem, resultando em desempenho subótimo tanto em termos de precisão quanto de eficiência.

A ausência de frameworks teóricos robustos para a integração de métodos híbridos constitui uma barreira significativa para a adoção generalizada dessas tecnologias. Além disso, a maioria das propostas não considera adequadamente os aspectos de escalabilidade e distribuição de carga computacional, fatores críticos para aplicações empresariais de grande porte.

\subsection{Objetivo e Contribuições}

Este trabalho propõe uma arquitetura multi-agente inovadora para sistemas RAG que emprega estratégia de recuperação híbrida em pipeline hierárquico. A metodologia proposta utiliza BM25 como filtro inicial de alta eficiência para redução do espaço de busca, seguido por reranqueamento através de modelos de embedding denso aplicados exclusivamente ao subconjunto pré-selecionado de documentos candidatos.

A arquitetura proposta visa atingir os seguintes objetivos específicos:

\begin{enumerate}
    \item Redução significativa dos custos computacionais através da aplicação seletiva de algoritmos de embedding denso apenas em subconjuntos relevantes;
    \item Preservação da qualidade semântica mediante combinação otimizada de múltiplos modelos de embedding complementares;
    \item Garantia de escalabilidade horizontal através de processamento hierárquico e estratégias de distribuição de carga;
    \item Manutenção da interpretabilidade do sistema preservando transparência nas decisões de recuperação e ranqueamento.
\end{enumerate}

A validação experimental demonstra que esta arquitetura híbrida mantém desempenho competitivo com sistemas puramente baseados em embeddings densos, enquanto reduz dramaticamente os requisitos de infraestrutura computacional e latência de resposta do sistema.

\subsection{Organização do Documento}

Este artigo está estruturado da seguinte forma: a Seção~\ref{sec:relacionados} 
apresenta uma revisão abrangente dos trabalhos relacionados em recuperação 
híbrida e sistemas RAG. A Seção~\ref{sec:metodologia} detalha a metodologia 
proposta, incluindo a especificação da arquitetura multi-agente e algoritmos 
de otimização. A Seção~\ref{sec:metodologia} descreve aspectos técnicos da 
implementação e configuração experimental.

A Seção~\ref{sec:metodologia} apresenta o desenho experimental, datasets 
utilizados e métricas de avaliação. A Seção~\ref{sec:resultados} analisa os 
resultados obtidos através de comparação sistemática com abordagens do estado 
da arte. Finalmente, a Seção~\ref{sec:conclusao} consolida as contribuições 
do trabalho e aponta direções promissoras para pesquisas futuras.
\begin{table}[htbp]
  \centering
  \caption{Análise Comparativa de Paradigmas de Recuperação em Sistemas RAG}
  \label{tab:comparacao-paradigmas}
  \begin{tabularx}{\textwidth}{| l | >{\raggedright\arraybackslash}X | >{\raggedright\arraybackslash}X | >{\raggedright\arraybackslash}X |}
    \hline
    \textbf{Dimensão de Análise} 
      & \textbf{Métodos Lexicais (BM25)} 
      & \textbf{Embeddings Densos} 
      & \textbf{Arquitetura Híbrida Proposta} \\
    \hline
    Fundamento Teórico
      & Correspondência lexical via TF-IDF com normalização de frequência
      & Similaridade semântica em espaços vetoriais de alta dimensionalidade
      & Pipeline hierárquico com filtragem lexical e reranqueamento semântico \\
    \hline
    Complexidade Algorítmica
      & $O(n \log n)$ para construção de índices invertidos
      & $O(n \times d)$ para inferência em lote, onde $d$ é a dimensionalidade
      & $O(n \log n) + O(k \times d)$ com $k \ll n$ documentos filtrados \\
    \hline
    Latência de Resposta
      & 1--5 milissegundos para consultas típicas
      & 50--200 milissegundos dependendo do hardware
      & 10--30 milissegundos com otimizações de pipeline \\
    \hline
    Requisitos de Memória
      & Índices esparsos armazenados preferencialmente em disco
      & Representações vetoriais densas mantidas em memória RAM
      & Estratégia híbrida com gerenciamento otimizado de memória \\
    \hline
    Estratégia de Escalabilidade
      & Paralelização horizontal em arquiteturas CPU convencionais
      & Processamento vertical em unidades GPU especializadas
      & Distribuição adaptativa de carga com balanceamento dinâmico \\
    \hline
    Nível de Interpretabilidade
      & Elevado através de scores explícitos por termo de consulta
      & Reduzido devido à natureza abstrata do espaço latente
      & Intermediário com pipeline transparente e rastreabilidade \\
    \hline
    Robustez Semântica
      & Limitada a correspondências lexicais exatas ou aproximadas
      & Elevada capacidade de captura de relações semânticas complexas
      & Otimizada através da combinação sinérgica de ambas abordagens \\
    \hline
    Custo Operacional
      & Reduzido com requisitos mínimos de infraestrutura
      & Elevado devido à necessidade de hardware especializado
      & Balanceado com otimização de recursos computacionais \\
    \hline
  \end{tabularx}
\end{table}

\FloatBarrier

\section{Trabalhos Relacionados}
\label{sec:relacionados}

Esta seção apresenta uma análise sistemática dos principais trabalhos em recuperação de informação híbrida e sistemas RAG, organizados por abordagem metodológica e contribuições técnicas. A revisão enfoca especialmente as estratégias de combinação entre métodos léxicos e densos, identificando lacunas que motivam a proposta deste trabalho.

\subsection{Recuperação Híbrida: Fundamentos e Evolução}

A combinação de métodos léxicos e semânticos em sistemas de recuperação tem sido explorada através de diferentes estratégias arquiteturais. \citet{Zhang2022} propõem algoritmos híbridos integrando BM25 com SBERT e RoBERTa, validados nos datasets MS MARCO Passage e MS MARCO Document. Embora demonstrem melhorias na eficácia de recuperação, os autores reconhecem limitações de eficiência que tornam seus algoritmos inadequados para sistemas RAG de larga escala.

Complementarmente, \citet{Lu2022} investigam a combinação de BM25 com T5 Embeddings, utilizando inicialmente MS MARCO e posteriormente avaliando em modo zero-shot com o benchmark BEIR. Os resultados evidenciam que abordagens híbridas superam métodos isolados em diversas tarefas, porém a metodologia se limita a um único modelo denso, não explorando o potencial de ensembles de embeddings.

\subsection{Modelos de Embedding Multilíngues}

O desenvolvimento de representações densas multilíngues representa um avanço significativo para sistemas RAG globais. \citet{Wang2024} descrevem o treinamento do modelo "multilingual-e5-large-instruct", utilizando um corpus de 1 bilhão de pares texto-documento para pré-treinamento contrastivo, complementado por 1 milhão de pares para aprendizado supervisionado. O modelo alcança até 97\% de precisão no benchmark MIRACL, superando alternativas como LaBSE, BGE e Cohere em tarefas multilíngues.

Contudo, a dependência de dados sintéticos para treinamento introduz riscos de viés e degradação de performance em domínios específicos. Esta limitação é particularmente relevante para aplicações RAG que demandam robustez across domínios diversos, sugerindo a necessidade de abordagens complementares.

\subsection{Ranqueamento Neural e Reranqueamento}

Estratégias de ranqueamento baseadas em modelos de linguagem de grande escala têm demonstrado eficácia na melhoria da precisão de recuperação. \citet{Ma2023} introduzem RepLLaMA e RankLLaMA para recuperação e reordenação de resultados por score, obtendo performance competitiva no benchmark BEIR. Todavia, o treinamento exclusivo em MS MARCO limita a generalização para problemas do mundo real, evidenciando a necessidade de estratégias de treinamento mais diversificadas.

A abordagem de \citet{Ma2023} concentra-se exclusivamente em métodos densos para ranqueamento, não explorando sinergias com algoritmos léxicos que poderiam reduzir custos computacionais mantendo qualidade.

\subsection{Arquiteturas RAG Especializadas}

\citet{Salemi2024} propõem uRAG, um algoritmo de treinamento padronizado para múltiplos domínios, priorizando facilidade de implementação e reutilização. Embora ofereça flexibilidade arquitetural, o sistema apresenta alta complexidade, baixa escalabilidade e latências inadequadas para aplicações em tempo real, limitando seu uso a ambientes internos controlados.

Esta limitação ressalta a importância de arquiteturas que equilibrem flexibilidade com eficiência operacional, especialmente para deployment em produção.

\subsection{Limitações de Abordagens Encoder-Decoder}

\citet{Wang2020} investigam combinações de arquiteturas Transformer, testando BERT como encoder e GPT-2 como decoder. Os experimentos revelam degradação de performance quando embeddings são gerados por sistemas distintos, sugerindo que a integração de modelos heterogêneos requer estratégias de harmonização cuidadosas.

Esta observação é particularmente relevante para sistemas multi-agente que empregam diferentes modelos de embedding, indicando a necessidade de técnicas de calibração e normalização.

\subsection{Lacunas e Oportunidades de Pesquisa}

A análise crítica dos trabalhos relacionados revela lacunas significativas no estado da arte que motivam o desenvolvimento desta pesquisa. Embora diversos estudos tenham explorado a combinação de métodos esparsos e densos em sistemas de recuperação de informação, a literatura apresenta limitações importantes que impedem a adoção efetiva dessas soluções em ambientes de produção.

A primeira lacuna identificada refere-se à orquestração eficiente de pipelines híbridos. Os trabalhos existentes concentram-se predominantemente na combinação algorítmica de diferentes métodos de recuperação, mas não desenvolvem estratégias sistematizadas para otimização da sequência de operações. Esta limitação resulta em implementações ad-hoc que não maximizam a eficiência computacional nem exploram adequadamente as sinergias entre componentes lexicais e semânticos.

A segunda lacuna relaciona-se ao equilíbrio entre qualidade semântica e escalabilidade operacional. As soluções propostas na literatura frequentemente priorizam a precisão dos resultados sem considerar adequadamente os custos computacionais e de infraestrutura associados ao deployment em larga escala. Esta abordagem limita a aplicabilidade prática dessas técnicas, especialmente em cenários que demandam processamento de grandes volumes de dados com restrições de latência.

A terceira lacuna identificada corresponde à ausência de arquiteturas multi-agente especificamente projetadas para sistemas RAG (Retrieval-Augmented Generation). Embora sistemas multi-agente tenham demonstrado eficácia em diversos domínios da inteligência artificial, sua aplicação em contextos de recuperação e geração aumentada permanece inexplorada, representando uma oportunidade significativa para otimização de recursos computacionais e melhoria da qualidade das respostas geradas.

Estas lacunas motivam o desenvolvimento de uma arquitetura híbrida multi-agente que integre eficiência lexical com precisão semântica, mantendo viabilidade operacional para implementação em ambientes de produção. A proposta visa contribuir para o avanço do estado da arte através de uma abordagem sistematizada que endereça simultaneamente as limitações identificadas na literatura atual.

\begin{table}[htbp]
  \centering
  \caption{Síntese Comparativa dos Trabalhos Relacionados}
  \label{tab:trabalhos-relacionados}
  \begin{tabularx}{\textwidth}{| l | >{\raggedright\arraybackslash}X | >{\raggedright\arraybackslash}X | >{\raggedright\arraybackslash}X |}
    \hline
    \textbf{Trabalho} 
      & \textbf{Abordagem} 
      & \textbf{Contribuições} 
      & \textbf{Limitações} \\
    \hline
    Zhang et al. (2022) \nocite{zhang2022hlatr}
      & BM25 + SBERT/RoBERTa
      & Validação em MS MARCO; melhoria de eficácia
      & Ineficiência computacional; inadequado para RAG escalável \\
    \hline
    Wang et al. (2024) \nocite{wang2024multilingual}
      & E5 multilíngue
      & 97\% precisão MIRACL; suporte multilíngue
      & Dependência de dados sintéticos; viés de domínio \\
    \hline
    Lu et al. (2022) \nocite{lu2022hyrr}
      & BM25 + T5 Embeddings
      & Avaliação zero-shot BEIR; superioridade híbrida
      & Limitado a modelo denso único \\
    \hline
    Ma et al. (2023) \nocite{ma2023llama}
      & RepLLaMA/Rank
      LLaMA
      & Ranqueamento neural eficaz
      & Treinamento limitado MS MARCO; só métodos densos \\
    \hline
    Salemi \& Zamani (2024) \nocite{salemi2024urag}
      & uRAG multi-domínio
      & Padronização e reutilização
      & Alta complexidade; baixa escalabilidade \\
    \hline
    Wang \& Chen (2020) \nocite{wang2020whatdo}
      & BERT encoder + GPT-2 decoder
      & Análise arquitetural Transformer
      & Degradação com modelos heterogêneos \\
    \hline
    \textbf{Este Trabalho}
      & \textbf{Multi-agente híbrido}
      & \textbf{Pipeline otimizado; escalabilidade}
      & \textbf{A ser validado experimentalmente} \\
    \hline
  \end{tabularx}
\end{table}

\FloatBarrier

\section{Fundamentos Teóricos}

Este capítulo apresenta os conceitos fundamentais que sustentam o desenvolvimento de sistemas de recuperação de informação baseados em inteligência artificial. A discussão abrange desde arquiteturas de agentes com modelos de linguagem até algoritmos específicos de ranqueamento e recuperação, fornecendo o embasamento teórico necessário para compreender as contribuições propostas neste trabalho.

\subsection{Arquiteturas de Agentes com Modelos de Linguagem}

O desenvolvimento de sistemas baseados em agentes de inteligência artificial tem evoluído para incorporar diferentes paradigmas arquiteturais, especialmente no contexto de modelos de linguagem de grande escala (LLMs). Duas abordagens principais emergem neste cenário: LangChain e arquiteturas baseadas em grafos (Graph-based architectures), cada uma oferecendo diferentes níveis de complexidade e capacidades de orquestração.

O LangChain representa uma abordagem linear e sequencial para a composição de agentes, onde a query do usuário é processada através de uma cadeia determinística de operações. Esta arquitetura utiliza documentos recuperados de bases de dados para construir prompts contextualizados, que são posteriormente processados por LLMs para gerar respostas finais. A simplicidade desta abordagem facilita a implementação e depuração, sendo adequada para cenários onde o fluxo de processamento é bem definido e previsível.

Em contraste, as arquiteturas baseadas em grafos implementam redes complexas de agentes interconectados, onde múltiplos agentes podem ser acionados de forma sequencial, paralela ou recursiva. Esta abordagem permite a criação de sistemas multi-agente sofisticados, onde agentes especializados (como pré-processadores de prompt, validadores de contexto, ou refinadores de resposta) colaboram para produzir resultados compostos. A flexibilidade desta arquitetura possibilita a implementação de fluxos de trabalho adaptativos e condicionais.

\subsection{Recuperação de Informação}

A recuperação de informação constitui um campo fundamental da ciência da computação que se dedica ao desenvolvimento de algoritmos e técnicas para localizar, ranquear e extrair informações relevantes de grandes coleções de documentos. O processo central envolve a comparação de similaridade entre queries de usuários e documentos candidatos, utilizando diferentes abordagens matemáticas e estatísticas para quantificar a relevância.

Os sistemas de recuperação de informação empregam duas categorias principais de algoritmos: métodos esparsos e métodos densos. Os métodos esparsos, exemplificados pelo BM25, baseiam-se em correspondências lexicais e estatísticas de frequência de termos, representando documentos e queries através de vetores de alta dimensionalidade com poucos elementos não-nulos. Os métodos densos, por sua vez, utilizam representações vetoriais aprendidas através de redes neurais, onde documentos e queries são mapeados para espaços vetoriais de menor dimensionalidade, mas com todos os elementos tipicamente não-nulos.

A eficácia dos sistemas de recuperação é tradicionalmente avaliada através de métricas como precisão, revocação, e medidas compostas como F1-score e MAP (Mean Average Precision). Estas métricas permitem quantificar tanto a capacidade do sistema de encontrar documentos relevantes quanto sua habilidade de evitar a recuperação de documentos irrelevantes, fornecendo uma base objetiva para comparação de diferentes abordagens algorítmicas.
\vspace{1em} % Espaço opcional antes da figura

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.38\textwidth]{Template_SBC/template-latex/images/general algorithm.png}
    \caption{Reranking de documentos.}
    \label{fig:your-label}
\end{figure}

\vspace{1em} 
\FloatBarrier

\subsection{Transformers}

A arquitetura Transformer representa uma mudança de paradigma fundamental na modelagem de sequências, introduzindo o mecanismo de auto-atenção como bloco computacional primário. A inovação central reside na geração simultânea de representações de consulta (Q), chave (K) e valor (V) a partir de sequências de entrada, o que permite ao modelo capturar relações complexas entre elementos independentemente de sua distância posicional.

\subsubsection{Mecanismo de Auto-Atenção}

O mecanismo de auto-atenção opera através da fórmula de atenção por produto escalar escalado:

\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

Esta formulação computa pesos de atenção medindo a compatibilidade entre consultas e chaves, normalizada pela raiz quadrada da dimensão da chave para prevenir gradientes extremamente pequenos na função softmax. Os pesos de atenção resultantes são então aplicados aos valores, criando uma representação ponderada que captura a importância relativa de diferentes elementos da sequência.

\subsubsection{Atenção Multi-Cabeça}

O mecanismo de atenção multi-cabeça estende a atenção básica computando múltiplas funções de atenção em paralelo. Cada cabeça de atenção aprende diferentes tipos de relações dentro dos dados, permitindo ao modelo atender informações de diferentes subespaços de representação simultaneamente. As saídas de todas as cabeças são concatenadas e transformadas linearmente para produzir a representação final.

\begin{equation}
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O
\end{equation}

onde $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$.

\subsubsection{Arquitetura Codificador-Decodificador}

O Transformer emprega uma estrutura codificador-decodificador distinta onde ambos componentes servem funções especializadas. O codificador processa a sequência de entrada e gera uma representação abrangente da informação fonte. Esta codificação captura relações contextuais e significado semântico através de múltiplas camadas de auto-atenção e redes feed-forward.

O decodificador opera sequencialmente, gerando tokens de saída enquanto atende tanto às representações do codificador quanto aos tokens previamente gerados. Crucialmente, o decodificador incorpora mecanismos de mascaramento para prevenir vazamento de informação de posições futuras durante o treinamento, garantindo que as predições dependam apenas do contexto disponível.

\subsubsection{Metodologia de Treinamento}

O processo de treinamento segue uma abordagem de aprendizado hierárquico. O codificador aprende a criar representações significativas dos dados de entrada através de objetivos de aprendizado supervisionado ou não-supervisionado. O decodificador subsequentemente aprende a gerar saídas apropriadas condicionadas às representações do codificador e ao contexto da sequência alvo.

Esta dependência sequencial de aprendizado reflete o princípio de design arquitetural: o codificador deve primeiro desenvolver capacidades efetivas de representação de dados antes que o decodificador possa aprender a produzir saídas coerentes. O codificador essencialmente aprende a compreender e codificar o domínio de entrada, enquanto o decodificador aprende a mapear essas representações codificadas para o domínio de saída desejado.

\subsubsection{Inovações Arquiteturais}

O design do Transformer elimina as restrições de processamento sequencial inerentes às arquiteturas recorrentes, permitindo computação paralela através das posições da sequência. Esta paralelização melhora significativamente a eficiência de treinamento mantendo a capacidade do modelo de capturar dependências de longo alcance através do mecanismo de atenção.

A flexibilidade da arquitetura provou-se efetiva através de domínios diversos, desde processamento de linguagem natural até visão computacional, demonstrando a universalidade da abordagem baseada em atenção para tarefas de modelagem de sequências.

\subsubsection{Codificação Posicional}

Dado que o mecanismo de atenção é invariante à permutação, o Transformer incorpora codificação posicional para fornecer informação sobre a ordem dos elementos na sequência:

\begin{align}
PE_{(pos,2i)} &= \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right) \\
PE_{(pos,2i+1)} &= \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\end{align}

onde $pos$ é a posição e $i$ é a dimensão. Esta codificação permite ao modelo distinguir entre diferentes posições na sequência mantendo a capacidade de generalização para sequências de comprimentos variados.

\cite{salemi2024urag}
\vspace{1em} % Espaço opcional antes da figura

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.38\textwidth]{Template_SBC/template-latex/images/transformers.png}
    \caption{Transformers}
    \label{fig:your-label}
\end{figure}

 
\FloatBarrier
\vspace{1em}
\subsection{BM25 (Best Matching 25)}

O BM25 foi proposto por \cite{robertson1994bm25} como uma função de ranqueamento probabilística para recuperação de informação, fundamentada no framework de independência binária. Este modelo representa uma evolução significativa dos métodos clássicos de recuperação, combinando princípios estatísticos com heurísticas empíricas para estimar a relevância de documentos.

O algoritmo computa a relevância através da combinação de três componentes principais: frequência de termos (TF) com saturação controlada, frequência inversa de documentos (IDF) para capturar a raridade dos termos, e normalização por comprimento de documento para evitar viés em favor de textos extensos. A função utiliza dois hiperparâmetros críticos: k₁, que controla a saturação da frequência de termos (tipicamente entre 1.2 e 2.0), e b, que regula a normalização por comprimento (geralmente 0.75).

O processamento das queries envolve tokenização, aplicação de stemming e remoção de stopwords, seguido pelo cálculo de scores através da soma ponderada dos termos individuais. Esta abordagem permite que o BM25 mantenha complexidade computacional linear enquanto oferece desempenho robusto em diversas coleções de documentos.

Atualmente, o BM25 permanece como algoritmo de referência em sistemas de recuperação de informação, sendo implementado em ferramentas amplamente utilizadas como Elasticsearch e Apache Lucene, devido à sua simplicidade computacional e eficácia empírica comprovada em múltiplos domínios.
\vspace{1em} % Espaço opcional antes da figura

%\begin{figure}[htbp]
%    \centering
%    \includegraphics[width=0.3\textwidth]%{Template_SBC/template-latex/images/BM25.png}
    %\caption{Sistema RAG (Retrieval Augmented Generation) baseline}
%    \label{fig:your-label}
%\end{figure}

%\vspace{1em} 
\section{Fundamentos Teóricos}

Este capítulo apresenta os conceitos fundamentais que sustentam o desenvolvimento de sistemas de recuperação de informação baseados em inteligência artificial. A discussão abrange desde arquiteturas de agentes com modelos de linguagem até algoritmos específicos de ranqueamento e recuperação, fornecendo o embasamento teórico necessário para compreender as contribuições propostas neste trabalho.

\subsection{Arquiteturas de Agentes com Modelos de Linguagem}

O desenvolvimento de sistemas baseados em agentes de inteligência artificial tem evoluído para incorporar diferentes paradigmas arquiteturais, especialmente no contexto de modelos de linguagem de grande escala (LLMs). Duas abordagens principais emergem neste cenário: LangChain e arquiteturas baseadas em grafos (Graph-based architectures), cada uma oferecendo diferentes níveis de complexidade e capacidades de orquestração.

O LangChain representa uma abordagem linear e sequencial para a composição de agentes, onde a query do usuário é processada através de uma cadeia determinística de operações. Esta arquitetura utiliza documentos recuperados de bases de dados para construir prompts contextualizados, que são posteriormente processados por LLMs para gerar respostas finais. A simplicidade desta abordagem facilita a implementação e depuração, sendo adequada para cenários onde o fluxo de processamento é bem definido e previsível.

Em contraste, as arquiteturas baseadas em grafos implementam redes complexas de agentes interconectados, onde múltiplos agentes podem ser acionados de forma sequencial, paralela ou recursiva. Esta abordagem permite a criação de sistemas multi-agente sofisticados, onde agentes especializados (como pré-processadores de prompt, validadores de contexto, ou refinadores de resposta) colaboram para produzir resultados compostos. A flexibilidade desta arquitetura possibilita a implementação de fluxos de trabalho adaptativos e condicionais.

\subsection{Recuperação de Informação}

A recuperação de informação constitui um campo fundamental da ciência da computação que se dedica ao desenvolvimento de algoritmos e técnicas para localizar, ranquear e extrair informações relevantes de grandes coleções de documentos. O processo central envolve a comparação de similaridade entre queries de usuários e documentos candidatos, utilizando diferentes abordagens matemáticas e estatísticas para quantificar a relevância.

Os sistemas de recuperação de informação empregam duas categorias principais de algoritmos: métodos esparsos e métodos densos. Os métodos esparsos, exemplificados pelo BM25, baseiam-se em correspondências lexicais e estatísticas de frequência de termos, representando documentos e queries através de vetores de alta dimensionalidade com poucos elementos não-nulos. Os métodos densos, por sua vez, utilizam representações vetoriais aprendidas através de redes neurais, onde documentos e queries são mapeados para espaços vetoriais de menor dimensionalidade, mas com todos os elementos tipicamente não-nulos.

\subsection{Algoritmos Densos}

Os algoritmos densos representam uma classe de métodos de recuperação de informação que utilizam representações vetoriais aprendidas para capturar similaridade semântica entre queries e documentos. Diferentemente dos métodos esparsos que dependem de correspondências lexicais exatas, os algoritmos densos mapeiam textos para espaços vetoriais contínuos de menor dimensionalidade, onde a proximidade geométrica reflete similaridade semântica.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.38\textwidth]{Template_SBC/template-latex/images/generalized-dense-algorithm.png}
    \caption{Arquitetura generalizada de algoritmos densos para recuperação de informação}
    \label{fig:dense-algorithm}
\end{figure}

A arquitetura típica dos algoritmos densos envolve a utilização de encoders neurais, frequentemente baseados em transformers, para processar tanto queries quanto documentos de forma independente. Este processamento resulta em embeddings de dimensão fixa que preservam informações semânticas contextuais. A similaridade entre query e documento é então computada através de métricas como similaridade cosseno ou produto escalar no espaço de embeddings.

O treinamento destes modelos geralmente emprega aprendizado contrastivo, onde o objetivo é maximizar a similaridade entre pares relevantes (query, documento) enquanto minimiza a similaridade entre pares irrelevantes. Esta abordagem permite que o modelo aprenda representações que capturam nuances semânticas que métodos baseados em correspondência lexical não conseguem identificar.

A principal vantagem dos algoritmos densos reside na sua capacidade de recuperar documentos semanticamente relevantes mesmo quando não há sobreposição lexical significativa entre query e documento. Esta característica é particularmente valiosa em cenários onde usuários formulam queries usando terminologia diferente da encontrada nos documentos relevantes, um fenômeno conhecido como vocabulary mismatch problem.
\subsection{SBERT (Sentence-BERT)}

O SBERT (Sentence-BERT) foi desenvolvido por \cite{reimers2019sbert} como uma modificação da arquitetura BERT que utiliza redes neurais siamesas para gerar embeddings semanticamente consistentes de sentenças. Esta abordagem foi proposta para resolver a limitação computacional do BERT em tarefas de similaridade de sentenças, onde a comparação de n sentenças requer O(n²) operações de inferência.

A arquitetura do SBERT emprega redes siamesas com parâmetros compartilhados, processando sentenças de entrada através de encoders BERT independentes seguidos por uma camada de pooling médio. O modelo utiliza BERT-base (110M parâmetros, 768 dimensões) ou BERT-large (340M parâmetros, 1024 dimensões) como backbone, produzindo representações vetoriais de dimensão fixa que preservam informações semânticas contextuais.

O treinamento do modelo incorpora três funções objetivo distintas: classificação através da concatenação de embeddings com diferença elemento-wise, regressão utilizando similaridade cosseno entre representações, e aprendizado contrastivo via triplet loss. Os dados de treinamento incluem os datasets SNLI (570k pares de sentenças) e MultiNLI (430k pares) para tarefas de inferência de linguagem natural, além do STS benchmark para similaridade textual semântica.

A principal contribuição do SBERT reside na redução da complexidade computacional de O(n²) para O(n) em tarefas de recuperação de informação, permitindo a pré-computação de embeddings de sentenças e possibilitando comparações eficientes através de operações de similaridade cosseno em tempo real.

\vspace{1em} % Espaço opcional antes da figura

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.38\textwidth]{Template_SBC/template-latex/images/SBERT.png}
    \caption{Arquitetura SBERT.}
    \label{fig:your-label}
\end{figure}

\vspace{1em} 
\FloatBarrier


\subsection{ColBERT (Contextualized Late Interaction over BERT)}

O ColBERT foi proposto por \cite{khattab2020colbert} como um modelo de recuperação neural que combina a eficácia de representações contextualizadas com a eficiência computacional através do paradigma de interação tardia. Este modelo foi desenvolvido para superar as limitações dos sistemas de recuperação baseados em representações densas, que sofrem com altos custos computacionais durante a fase de inferência.

A arquitetura do ColBERT utiliza BERT-base (110M parâmetros) como encoder backbone, processando queries e documentos de forma independente para gerar representações contextualizadas em nível de token. O modelo aplica uma camada de projeção linear para reduzir a dimensionalidade dos embeddings de 768 para 128 dimensões, seguida por normalização L2 para estabilização numérica. Esta abordagem permite que representações de documentos sejam pré-computadas e indexadas offline.

O mecanismo de pontuação emprega a função MaxSim, que computa a similaridade entre query e documento através do máximo da similaridade cosseno entre cada token da query e todos os tokens do documento. O score final é obtido pela soma das pontuações MaxSim de todos os tokens da query, permitindo correspondências fine-grained que capturam relações semânticas locais entre termos.

O modelo foi treinado no dataset MS MARCO (533k pares query-documento) utilizando aprendizado contrastivo in-batch com negative sampling. Esta estratégia de late interaction reduz significativamente os custos computacionais durante a recuperação, mantendo a qualidade das representações contextualizadas e possibilitando escalabilidade para coleções de documentos de grande escala.
\vspace{1em} % Espaço opcional antes da figura

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.38\textwidth]{Template_SBC/template-latex/images/colbert.png}
    \caption{colbert}
    \label{fig:your-label}
\end{figure}
\vspace{1em} 
\FloatBarrier


\subsection{Multilingual E5-Large-Instruct}

O modelo \texttt{multilingual-e5-large-instruct} representa uma implementação avançada de embedding textual desenvolvida pela Microsoft Research. Este modelo foi especificamente projetado para recuperação de informação multilíngue e tarefas de similaridade semântica. Constitui uma extensão da família E5 (Embeddings from bidirectional Encoder representations), incorporando capacidades de instrução que permitem maior flexibilidade na geração de representações vetoriais contextualizadas.

\subsubsection{Arquitetura e Fundamentos Técnicos}

O modelo baseia-se na arquitetura Transformer, utilizando especificamente uma variante do encoder BERT (Bidirectional Encoder Representations from Transformers). A arquitetura apresenta modificações específicas para otimização em tarefas de embedding. A estrutura fundamental emprega 24 camadas de transformação com mecanismo de auto-atenção multi-cabeça, dimensionalidade de 1024 para representação oculta ($d_{model} = 1024$), 16 cabeças por camada de atenção ($h = 16$) e aproximadamente 560 milhões de parâmetros treináveis.

\subsubsection{Processo de Tokenização e Embedding}

O pipeline de processamento opera em duas etapas principais. A primeira etapa consiste na tokenização automática utilizando o tokenizador SentencePiece, que processa consultas e documentos de entrada. Este processo converte texto bruto em sequências de tokens numéricos compatíveis com a arquitetura do modelo.

A segunda etapa envolve a geração de embeddings através do processamento das sequências tokenizadas pelo encoder Transformer. Este processo gera representações vetoriais densas e pode ser formalizado como $\mathbf{e} = \text{Pooling}(\text{Transformer}(\text{Tokenize}(\text{input})))$. O pooling utiliza estratégias como mean pooling ou CLS token pooling para produzir representações de dimensão fixa.

\subsubsection{Dados de Treinamento e Metodologia}

O modelo foi treinado em um corpus multilíngue extensivo que inclui mais de 1 bilhão de pares texto-texto de múltiplos idiomas. Os dados abrangem textos acadêmicos, web crawling, documentação técnica e literatura. O treinamento incorpora datasets sintéticos e humanos para fine-tuning instrucional, oferecendo suporte para mais de 100 idiomas diferentes.

\subsubsection{Capacidades de Processamento}

O modelo apresenta especificações operacionais bem definidas. O comprimento máximo de sequência é de 512 tokens, com dimensão do embedding de saída de 1024. O vocabulário possui 250.000 tokens e oferece suporte para mais de 100 idiomas. O tamanho total do modelo é de 2,24 GB, conforme detalhado na Tabela~\ref{tab:e5_specs}.

\begin{table}[h]
\centering
\caption{Especificações técnicas do modelo multilingual-e5-large-instruct}
\label{tab:e5_specs}
\begin{tabular}{|l|r|}
\hline
\textbf{Especificação} & \textbf{Valor} \\
\hline
Comprimento máximo de sequência & 512 tokens \\
Dimensão do embedding de saída & 1024 \\
Vocabulário & 250.000 tokens \\
Idiomas suportados & 100+ \\
Tamanho do modelo & 2,24 GB \\
\hline
\end{tabular}
\end{table}

\subsubsection{Inovações Metodológicas}

O modelo incorpora treinamento instrucional que o diferencia de modelos de embedding tradicionais. O E5-large-instruct foi treinado com prefixos instrucionais que permitem controle explícito sobre o tipo de representação desejada. Esta abordagem melhora significativamente a adaptabilidade a diferentes tarefas downstream.

A arquitetura foi especificamente otimizada para manter consistência semântica entre idiomas. Esta otimização utiliza técnicas de alinhamento cross-lingual durante o treinamento. O processo emprega regularização contrastiva através de loss functions que maximizam a similaridade entre representações semanticamente relacionadas.

A função de perda contrastiva é definida como:
\begin{equation}
\mathcal{L} = -\log \frac{\exp(\text{sim}(q, d^+)/\tau)}{\sum_{d^- \in D^-} \exp(\text{sim}(q, d^-)/\tau)}
\end{equation}
onde $q$ representa a consulta, $d^+$ o documento positivo, $D^-$ o conjunto de documentos negativos, e $\tau$ o parâmetro de temperatura.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.38\textwidth]{Template_SBC/template-latex/images/E5.png}
    \caption{Arquitetura do modelo E5}
    \label{fig:e5_architecture}
\end{figure}

\subsection{GTE-QWEN2}

O modelo GTE-QWEN2 (General Text Embedding - QWEN2) representa uma implementação avançada de embeddings textuais desenvolvida pela Alibaba Cloud. Constitui uma evolução significativa da família QWEN (通义千问) de modelos de linguagem. Este sistema foi projetado especificamente para tarefas de recuperação de informação e similaridade semântica, integrando-se ao ecossistema de inteligência artificial da Alibaba.

\subsubsection{Arquitetura e Fundamentos}

O GTE-QWEN2 baseia-se em uma arquitetura Transformer otimizada, derivada dos avanços do modelo base QWEN2. A implementação utiliza arquitetura Transformer decoder-only com modificações específicas para tarefas de embedding. O mecanismo de atenção incorpora Grouped Query Attention (GQA) para eficiência computacional, RMSNorm em substituição ao LayerNorm tradicional, e função de ativação SwiGLU para melhor expressividade não-linear.

\subsubsection{Pipeline de Processamento}

O processamento textual no GTE-QWEN2 segue um pipeline estruturado de transformação. Este pipeline pode ser formalizado como $\mathbf{E} = f_{emb}(f_{trans}(f_{token}(\mathbf{T})))$, onde $f_{token}(\mathbf{T})$ representa a função de tokenização do texto de entrada. $f_{trans}(\cdot)$ corresponde às transformações através das camadas Transformer, e $f_{emb}(\cdot)$ representa a projeção final para o espaço de embedding.

O processo inicial envolve o mapeamento numérico através da conversão do texto de entrada em representações numéricas. O tokenizador emprega uma abordagem baseada em Byte Pair Encoding (BPE) otimizada para textos multilíngues. Esta otimização apresenta particular ênfase em idiomas asiáticos, garantindo melhor cobertura linguística.

Após a tokenização numérica, as sequências são processadas através das camadas Transformer. Cada token é transformado em representações vetoriais densas durante este processamento. O modelo aplica mecanismos de atenção para capturar dependências contextuais e gerar embeddings semanticamente ricos.

\subsubsection{Especificações Técnicas}

O modelo apresenta características operacionais variáveis conforme a configuração utilizada. O número de camadas Transformer varia entre 24 e 40, com dimensões de embedding de 768, 1024 ou 1536. O número de cabeças de atenção pode ser 12, 16 ou 24, dependendo da configuração específica. O tamanho do vocabulário é de 151.936 tokens, conforme detalhado na Tabela~\ref{tab:gte_specs}.

\begin{table}[h]
\centering
\caption{Especificações da família GTE-QWEN2}
\label{tab:gte_specs}
\begin{tabular}{|l|r|}
\hline
\textbf{Parâmetro} & \textbf{Especificação} \\
\hline
Camadas Transformer & 24-40 (variável) \\
Dimensão de embedding & 768/1024/1536 \\
Cabeças de atenção & 12/16/24 \\
Tamanho do vocabulário & 151.936 tokens \\
Comprimento máximo de contexto & 8.192/32.768 tokens \\
Parâmetros totais & 0,5B-7B \\
\hline
\end{tabular}
\end{table}
\FloatBarrier
\subsubsection{Metodologia de Treinamento}

O desenvolvimento do GTE-QWEN2 seguiu uma abordagem de treinamento em múltiplas etapas. O pré-treinamento utilizou um corpus textual massivo incluindo textos web multilíngues com ênfase em chinês e inglês. Os dados também abrangem literatura científica e técnica, documentação e manuais técnicos, além de dados estruturados e semi-estruturados.

A segunda fase focou especificamente em tarefas de embedding através de fine-tuning. Esta fase utiliza a função de perda contrastiva definida como:
\begin{equation}
\mathcal{L}_{contrastive} = -\log \frac{\exp(\text{sim}(\mathbf{q}, \mathbf{d}^+)/\tau)}{\sum_{i} \exp(\text{sim}(\mathbf{q}, \mathbf{d}_i)/\tau)}
\end{equation}
onde $\mathbf{q}$ representa a query, $\mathbf{d}^+$ o documento positivo, e $\tau$ o parâmetro de temperatura para controle da distribuição softmax.

\subsubsection{Otimizações Implementadas}

O GTE-QWEN2 incorpora várias otimizações específicas para eficiência computacional. O Grouped Query Attention reduz a complexidade computacional mantendo a qualidade das representações. Esta técnica funciona através do compartilhamento de parâmetros entre cabeças de atenção relacionadas, resultando em menor uso de memória e processamento mais eficiente.

A quantização adaptativa implementa técnicas de quantização post-training que reduzem o tamanho do modelo. Esta abordagem mantém a performance mesmo com redução significativa de recursos. A técnica é especialmente relevante para deployment em ambientes com restrições de recursos computacionais.

O pooling estratégico utiliza estratégias otimizadas que agregam informações de múltiplas posições da sequência. Esta agregação produz representações finais mais robustas através da combinação de diferentes estratégias. A representação final é calculada como:
\begin{equation}
\mathbf{e}_{final} = \alpha \cdot \text{mean\_pool}(\mathbf{H}) + \beta \cdot \text{cls\_token}(\mathbf{H}) + \gamma \cdot \text{max\_pool}(\mathbf{H})
\end{equation}
onde $\mathbf{H}$ representa as representações ocultas das camadas finais, e $\alpha$, $\beta$, $\gamma$ são pesos aprendíveis.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.38\textwidth]{Template_SBC/template-latex/images/GTE-QWEN2.png}
    \caption{Arquitetura do modelo GTE-QWEN2}
    \label{fig:gte_architecture}
\end{figure}
\FloatBarrier
\section{Metodologia}
\label{sec:metodologia}
Esta seção apresenta a metodologia experimental empregada para avaliar comparativamente algoritmos de recuperação de informação e reranqueamento de passagens. O estudo foi estruturado em múltiplas etapas sequenciais, implementando uma abordagem híbrida que combina métodos tradicionais de recuperação esparsa com técnicas modernas de embedding denso. O objetivo principal é demonstrar a eficácia da combinação dessas abordagens para melhorar a qualidade da recuperação de informação.

\subsection{Dataset e Preparação dos Dados}

O experimento utilizou o dataset MS MARCO Passages, uma coleção amplamente reconhecida para avaliação de sistemas de recuperação de informação. Especificamente, foram extraídas as primeiras 10.000 queries do conjunto de dados, juntamente com suas respectivas passagens relevantes. Esta amostra representa um subconjunto significativo que permite avaliação robusta mantendo viabilidade computacional. As características do dataset estão detalhadas na Tabela~\ref{tab:dataset_specs}.

\begin{table}[h]
\centering
\caption{Características do subconjunto do MS MARCO Passages utilizado}
\label{tab:dataset_specs}
\begin{tabular}{|l|r|}
\hline
\textbf{Componente} & \textbf{Quantidade} \\
\hline
Total de queries & 10.000 \\
Passagens por query (média) & 1.000 \\
Tamanho médio das passagens & 56 tokens \\
Tamanho médio das queries & 6 tokens \\
Idioma predominante & Inglês \\
\hline
\end{tabular}
\end{table}

\subsection{Pipeline de Recuperação Híbrida}

A metodologia implementou um pipeline de recuperação em múltiplas etapas, conforme ilustrado na Figura~\ref{fig:pipeline_hybrid}. Este approach visa otimizar tanto a eficiência computacional quanto a qualidade dos resultados através de refinamentos sucessivos. O pipeline combina a eficiência computacional da recuperação esparsa com a qualidade semântica da recuperação densa. Esta combinação permite processar grandes corpora mantendo alta precisão nos resultados finais.

\subsubsection{Etapa 1: Recuperação Inicial com BM25}

A primeira etapa empregou o algoritmo BM25 como método de recuperação inicial, conforme descrito no Algoritmo~\ref{alg:bm25}. O BM25 foi configurado com parâmetros padrão $k_1 = 1.2$ e $b = 0.75$, amplamente utilizados na literatura. Esta configuração oferece um bom equilíbrio entre correspondência exata de termos e normalização por comprimento de documento. O algoritmo é responsável pela recuperação eficiente de um conjunto candidato de 1.000 passagens por query.

\begin{center}
\begin{framed}
\noindent
\begin{algorithm}[H]
\caption{Recuperação Inicial BM25}
\label{alg:bm25}
\KwIn{Consulta $Q = \{q_1, q_2, \dots, q_n\}$, Corpus $\mathcal{C}$, Parâmetros $k_1$, $b$, Top $M$}
\KwOut{Top-$M$ documentos candidatos ranqueados por BM25}
\BlankLine
\textbf{Inicialização:}\\
\quad $N \gets |\mathcal{C}|$ \tcp*{Número total de documentos} \\
\quad $\mathit{avgdl} \gets \frac{1}{N} \sum_{D \in \mathcal{C}} |D|$ \tcp*{Comprimento médio dos documentos} \\
\quad $\mathit{score}[D] \gets 0$ para todo $D \in \mathcal{C}$
\BlankLine
\textbf{Processamento por Termo:}\\
\ForEach{$q_i \in Q$}{
    $n_i \gets |\{D \in \mathcal{C} : f_{i,D} > 0\}|$ \tcp*{Documentos contendo $q_i$} \\
    $\mathrm{IDF}_i \gets \ln\!\left(\frac{N - n_i + 0.5}{n_i + 0.5}\right)$ \\
    \ForEach{$D \in \mathcal{C}$}{
        $f_{i,D} \gets \text{frequência de }q_i\text{ em }D$ \\
        $\mathrm{TF}_{i,D} \gets \frac{f_{i,D} \cdot (k_1 + 1)}{f_{i,D} + k_1 \cdot \left(1 - b + b \cdot \frac{|D|}{\mathit{avgdl}}\right)}$ \\
        $\mathit{score}[D] \mathrel{+}= \mathrm{IDF}_i \times \mathrm{TF}_{i,D}$
    }
}
\BlankLine
\textbf{Seleção de Candidatos:}\\
\quad Ordenar documentos por $\mathit{score}[D]$ em ordem decrescente \\
\quad \Return Top-$M$ documentos como conjunto candidato $\mathcal{B}$
\end{algorithm}
\end{framed}
\end{center}

\subsubsection{Etapa 2: Reranqueamento com Embedding Denso}

O conjunto candidato retornado pelo BM25 foi submetido a reranqueamento utilizando modelos de embedding denso. Esta etapa aplica os modelos Multilingual E5-Large-Instruct e GTE-QWEN2 descritos anteriormente. O processo de reranqueamento é formalizado no Algoritmo~\ref{alg:dense_rerank}, que processa os candidatos através de embeddings contextualizados. A similaridade coseno é utilizada como métrica de relevância semântica entre query e documentos.

\begin{center}
\begin{framed}
\noindent
\begin{algorithm}[H]
\caption{Reranqueamento com Embedding Denso}
\label{alg:dense_rerank}
\KwIn{Consulta $Q$, Conjunto candidato $\mathcal{B}$, Modelo de embedding $\mathcal{M}$, Top $K$}
\KwOut{Top-$K$ documentos reranqueados por similaridade semântica}
\BlankLine
\textbf{Geração de Embeddings:}\\
\quad $\mathbf{e}_Q \gets \mathcal{M}.\text{encode}(Q)$ \tcp*{Embedding da consulta} \\
\quad $\mathcal{E}_D \gets \{\}$ \tcp*{Conjunto de embeddings de documentos} \\
\ForEach{$D \in \mathcal{B}$}{
    $\mathbf{e}_D \gets \mathcal{M}.\text{encode}(D)$ \tcp*{Embedding do documento} \\
    $\mathcal{E}_D[\text{id}(D)] \gets \mathbf{e}_D$
}
\BlankLine
\textbf{Cálculo de Similaridade:}\\
\ForEach{$D \in \mathcal{B}$}{
    $\mathbf{e}_D \gets \mathcal{E}_D[\text{id}(D)]$ \\
    $\text{score}_{\text{dense}}[D] \gets \frac{\mathbf{e}_Q \cdot \mathbf{e}_D}{|\mathbf{e}_Q| \cdot |\mathbf{e}_D|}$ \tcp*{Similaridade coseno}
}
\BlankLine
\textbf{Ranking Final:}\\
\quad Ordenar $\mathcal{B}$ por $\text{score}_{\text{dense}}[D]$ em ordem decrescente \\
\quad \Return Top-$K$ documentos reranqueados
\end{algorithm}
\end{framed}
\end{center}

\subsubsection{Configuração Experimental}

Os experimentos foram configurados com os seguintes parâmetros. A recuperação inicial com BM25 retorna 1.000 candidatos por query ($M = 1000$). O reranqueamento denso processa estes candidatos e retorna os top-100 documentos mais relevantes ($K = 100$). Esta configuração oferece equilíbrio entre qualidade dos resultados e eficiência computacional. Os modelos de embedding foram executados com precisão float32 para garantir consistência nos resultados.

\subsection{Ambiente Computacional}

O experimento foi executado em ambiente computacional híbrido para lidar com os diferentes requisitos algorítmicos. O processamento BM25 utilizou CPU single-thread devido à natureza sequencial do algoritmo. Os modelos de embedding foram distribuídos across múltiplas GPUs para acelerar o processamento paralelo. O ambiente de desenvolvimento consistiu em Jupyter Notebooks para prototipagem iterativa e documentação inline. Esta configuração permitiu flexibilidade no desenvolvimento e reprodutibilidade dos experimentos.

\subsection{Métricas de Avaliação}

A qualidade dos algoritmos foi avaliada utilizando métricas padrão de recuperação de informação. As métricas principais incluem Mean Reciprocal Rank (MRR), Recall e Normalized Discounted Cumulative Gain (NDCG). Cada métrica foi calculada para diferentes profundidades de ranking ($k \in \{1, 5, 10, 20\}$). Esta análise granular permite compreender o comportamento dos algoritmos em diferentes cenários de uso.

As fórmulas das métricas são definidas como:
\begin{align}
\text{MRR@k} &= \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i} \label{eq:mrr}\\
\text{Recall@k} &= \frac{\text{documentos relevantes recuperados em k}}{\text{total de documentos relevantes}} \label{eq:recall}\\
\text{NDCG@k} &= \frac{DCG@k}{IDCG@k} \label{eq:ndcg}
\end{align}

\subsection{Validação Experimental}

A validação dos resultados seguiu protocolo rigoroso com múltiplas verificações. O baseline foi estabelecido através de comparação com resultados publicados no MS MARCO leaderboard. Estudos de ablação foram realizados para analisar a contribuição individual de cada componente do pipeline. Testes de significância estatística foram aplicados para validar as diferenças observadas entre os algoritmos. Métricas de eficiência computacional incluindo latência e throughput foram coletadas para cada algoritmo.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{Template_SBC/template-latex/images/b.png}
    \caption{Pipeline de recuperação híbrida multi-estágio}
    \label{fig:pipeline_hybrid}
\end{figure}

O processo experimental foi projetado para isolar o impacto de cada algoritmo na qualidade final dos resultados. Esta abordagem permite análise comparativa robusta entre diferentes estratégias de recuperação e reranqueamento. A metodologia garante reprodutibilidade através de configurações padronizadas e documentação detalhada de todos os parâmetros experimentais.

\section{Resultados e Discussão}
\label{sec:resultados}
Esta seção apresenta os resultados experimentais obtidos através da
avaliação comparativa de algoritmos de recuperação de informação
\textit{standalone}, seguida de uma análise crítica das implicações
destes resultados para o campo de recuperação de informação em sistemas
multiagentes de inteligência artificial.

\subsection{\textit{Performance} Comparativa dos Algoritmos}

Os experimentos realizados revelaram diferenças significativas na
\textit{performance} dos algoritmos avaliados, conforme apresentado
nas Tabelas~\ref{tab:ndcg-k}, \ref{tab:recall-precision} e \ref{tab:mrr-map}.
A análise abrangente incluiu métricas de NDCG@k, \textit{Recall}@k,
\textit{Precision}@k, MRR e MAP para os algoritmos SBERT, ColBERT,
E5 e BM25.

\subsection{Análise da \textit{Performance} do BM25}

Os resultados experimentais confirmam que o algoritmo BM25 apresenta
\textit{performance} inferior quando comparado aos métodos de \textit{embedding}
denso contemporâneos. Especificamente, o BM25 alcançou MRR de 0.3230
e MAP de 0.3172, representando aproximadamente 53\% da \textit{performance}
do algoritmo SBERT, que obteve os melhores resultados gerais.

O BM25 demonstrou \textit{Recall}@100 de 0.8135, indicando capacidade
limitada de recuperação de documentos relevantes em comparação com
algoritmos baseados em \textit{embeddings} densos, que alcançaram
valores próximos a 1.0000. Esta limitação posiciona o BM25 como
subótimo para aplicações em ambientes multiagentes de IA contemporâneos.

\subsection{Superioridade dos Algoritmos Baseados em \textit{Embeddings}}

Os algoritmos baseados em \textit{embeddings} densos demonstraram
superioridade consistente em todas as métricas avaliadas. O SBERT
alcançou os melhores resultados com MRR de 0.6049 e MAP de 0.6003,
seguido pelo E5 com MRR de 0.5888 e MAP de 0.5850. Estes resultados
confirmam a eficácia das representações semânticas densas para
recuperação de informação.

O ColBERT, apesar de utilizar \textit{embeddings} densos, apresentou
\textit{performance} intermediária com MRR de 0.3076, posicionando-se
entre os algoritmos tradicionais e os modelos de \textit{embedding}
mais sofisticados. Esta variação sugere que a arquitetura específica
do modelo de \textit{embedding} influencia significativamente os
resultados obtidos.

\subsection{Análise de NDCG@k}

A análise do NDCG@k revela padrões consistentes de \textit{performance}
entre os diferentes algoritmos. O SBERT mantém liderança em todas
as posições k avaliadas, com NDCG@10 de 0.6863 e NDCG@100 de 0.6970.
O E5 apresenta \textit{performance} comparável, especialmente em
posições superiores do \textit{ranking}, com NDCG@10 de 0.6737.

O BM25 demonstra NDCG@10 de 0.3856, representando uma diferença
substancial em relação aos algoritmos baseados em \textit{embeddings}.
Esta disparidade indica limitações significativas na qualidade do
\textit{ranking} produzido pelo algoritmo esparso tradicional.

\subsection{Comportamento de \textit{Recall} e \textit{Precision}}

Os resultados de \textit{Recall}@k demonstram que algoritmos baseados
em \textit{embeddings} alcançam cobertura quase completa dos documentos
relevantes nas primeiras 100 posições. SBERT e E5 atingem \textit{Recall}@100
de 1.0000, enquanto o BM25 alcança apenas 0.8135. Esta diferença
evidencia limitações fundamentais dos métodos esparsos em cenários
de alta precisão.

A análise de \textit{Precision}@k revela degradação esperada com
o aumento de k para todos os algoritmos. Contudo, SBERT mantém
\textit{Precision}@1 de 0.4235, significativamente superior ao BM25
com 0.1810. Esta diferença na precisão inicial tem implicações
críticas para sistemas RAG que dependem dos primeiros resultados
recuperados.

\subsection{Aplicabilidade em Cenários de Grande Escala}

Apesar das limitações identificadas, o BM25 demonstra vantagens
específicas em contextos caracterizados por extensividade documental.
Em cenários com \textit{corpus} documentais extremamente extensos
(superiores a 10⁷ documentos), o BM25 oferece capacidades de cobertura
que algoritmos densos podem não conseguir igualar devido a restrições
computacionais.

A complexidade linear do BM25 em relação ao tamanho do \textit{corpus}
contrasta favoravelmente com a complexidade dos métodos de \textit{embedding}
denso. Esta eficiência computacional representa vantagem significativa
em aplicações de grande escala, especialmente quando recursos
computacionais são limitados.

\subsection{Implicações para Sistemas Multiagentes}

Em contextos de sistemas multiagentes de IA, os resultados sugerem
estratégias de implementação diferenciadas baseadas nos requisitos
específicos da aplicação. Sistemas que requerem máxima precisão
devem empregar algoritmos baseados em \textit{embeddings} densos,
particularmente SBERT ou E5, para otimização da qualidade dos
resultados.

Cenários de recursos limitados podem beneficiar-se do BM25 como
solução de compromisso, aceitando redução na qualidade em favor
da eficiência computacional. Esta abordagem é particularmente
relevante em implementações distribuídas onde o custo computacional
é fator crítico de decisão.

\subsection{Implementação de \textit{Pipelines} Híbridos}

A integração de algoritmos esparsos e densos em \textit{pipelines}
híbridos representa direção promissora para otimização de sistemas
de recuperação. A combinação das vantagens de cobertura do BM25
com a precisão semântica de algoritmos baseados em \textit{embeddings}
pode resultar em arquiteturas superiores para aplicações práticas.

Os resultados detalhados da implementação e avaliação de \textit{pipelines}
híbridos serão apresentados em versão estendida deste trabalho.
A análise preliminar sugere potencial significativo para melhorias
na \textit{performance} geral através da combinação sinérgica de
diferentes abordagens algorítmicas.

\subsection{Limitações e Direções Futuras}

Os resultados apresentados estão sujeitos a limitações metodológicas
específicas. A avaliação foi conduzida utilizando conjunto de dados
MS MARCO, limitando a generalização para outros domínios. A análise
focou em domínio de língua inglesa, não considerando aspectos de
recuperação multilíngue.

Trabalhos futuros devem investigar a escalabilidade dos algoritmos
avaliados em \textit{corpus} de maior magnitude, bem como a aplicabilidade
em domínios especializados. A integração de técnicas de aprendizado
por reforço para otimização automática de parâmetros representa
direção promissora de pesquisa para o campo.

\begin{table}[ht]
\centering
\caption{NDCG@k para diferentes algoritmos de recuperação de informação}
\label{tab:ndcg-k}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccccccc@{}}
\toprule
\textbf{Algoritmo} & \textbf{@1} & \textbf{@3} & \textbf{@5} & \textbf{@8} & \textbf{@10} & \textbf{@20} & \textbf{@30} & \textbf{@40} & \textbf{@50} & \textbf{@100} \\
\midrule
SBERT     & 0.4235 & 0.5984 & 0.6516 & --     & 0.6863    & 0.6942     & --     & --     & 0.6968    & 0.6970    \\
ColBERT   & 0.1582 & 0.2683 & 0.3185 & --     & 0.3709    & 0.4045     & --     & --     & 0.4340    & 0.4491    \\
E5        & 0.4012 & 0.5818 & 0.6365 & 0.6551 & 0.6737    & 0.6821     & 0.6835 & 0.6842 & 0.6848    & 0.6851    \\
BM25      & 0.1810 & 0.2940 & 0.3423 & 0.3650 & 0.3856    & 0.4060     & 0.4135 & 0.4173 & 0.4211    & 0.4272    \\
\bottomrule
\end{tabular}%
}
\end{table}

\begin{table}[ht]
\centering
\caption{\textit{Recall}@k e \textit{Precision}@k para diferentes algoritmos}
\label{tab:recall-precision}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lccccccc|ccccccc@{}}
\toprule
\textbf{Algoritmo} & \multicolumn{7}{c|}{\textbf{\textit{Recall}@k}} & \multicolumn{7}{c}{\textbf{\textit{Precision}@k}} \\
\cmidrule(lr){2-8} \cmidrule(lr){9-15}
 & @1      & @3      & @5      & @10     & @20     & @50     & @100    & @1      & @3      & @5      & @10     & @20     & @50     & @100    \\
\midrule
SBERT     & 0.4122  & 0.7248  & 0.8527  & 0.9559  & 0.9860  & 0.9985  & 1.0000  & 0.4235  & 0.2504  & 0.1775  & 0.0999  & 0.0516  & 0.0209  & 0.0105  \\
ColBERT   & 0.1533  & 0.3500  & 0.4709  & 0.6307  & 0.7618  & 0.9084  & 1.0000  & 0.1582  & 0.1210  & 0.0981  & 0.0658  & 0.0398  & 0.0190  & 0.0105  \\
E5        & 0.3899  & 0.7118  & 0.8428  & 0.9534  & 0.9856  & 0.9984  & 1.0000  & 0.4012  & 0.2459  & 0.1755  & 0.0997  & 0.0516  & 0.0209  & 0.0105  \\
BM25      & 0.1740  & 0.3776  & 0.4941  & 0.6241  & 0.7025  & 0.7769  & 0.8135  & 0.1810  & 0.1314  & 0.1033  & 0.0655  & 0.0370  & 0.0164  & 0.0086  \\
\bottomrule
\end{tabular}%
}
\end{table}

\begin{table}[ht]
\centering
\caption{MRR e MAP dos algoritmos}
\label{tab:mrr-map}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Algoritmo} & \textbf{MRR} & \textbf{MAP} \\
\midrule
SBERT     & 0.6049 & 0.6003 \\
ColBERT   & 0.3076 & 0.3041 \\
E5        & 0.5888 & 0.5850 \\
BM25      & 0.3230 & 0.3172 \\
\bottomrule
\end{tabular}
\end{table}

\FloatBarrier

\section{Cronograma}

Os algoritmos do GraphChang utilizam sistemas multiagentes para execução
de tarefas complexas. Atualmente, observa-se um crescimento significativo
na quantidade de algoritmos e sistemas que dependem de múltiplos agentes
para realizar diversas operações computacionais. Nesse contexto emergente,
torna-se fundamental desenvolver agentes otimizados com capacidade de
utilização eficiente de contexto a baixo custo operacional.

Sob essa perspectiva, é essencial considerar novas técnicas heurísticas
que otimizem componentes específicos de sistemas LLM (\textit{Large Language
Model}) de forma computacionalmente eficiente. Em particular, no nível de
\textit{embeddings}, existem diversas otimizações possíveis relacionadas
ao treinamento desses modelos. Tais otimizações podem reduzir o custo
final do sistema mantendo o mesmo nível de eficiência operacional.

Os \textit{embeddings} apresentam custo computacional elevado para
reprocessamento sempre que um sistema mais robusto é desenvolvido.
Consequentemente, este trabalho busca, através de métodos esparsos,
acelerar o processo de geração de \textit{embeddings}. Para tanto,
serão utilizados conjuntos de teste (MS MARCO) e algoritmos esparsos
em combinação com algoritmos densos em arquitetura serial.

Nesse sentido, o algoritmo proposto será híbrido, combinando diferentes
abordagens computacionais. Utilizará métodos esparsos para tarefas de
maior escala e algoritmos densos para otimização da precisão dos
resultados obtidos. Esta estratégia híbrida permite balancear eficiência
computacional com qualidade dos resultados finais.

O algoritmo híbrido desenvolvido visa diminuir o custo de recuperação
de documentos em sistemas RAG (\textit{Retrieval-Augmented Generation}).
Isso ocorre porque algoritmos esparsos utilizam predominantemente CPU
e SSD, enquanto modelos densos exigem GPU, CPU e maior consumo energético.
Dessa forma, os métodos esparsos reduzem a quantidade de recursos
computacionais alocados para processamento de grandes volumes de dados.

Embora os métodos esparsos não entreguem necessariamente o melhor
resultado possível, podem gerar resultados satisfatórios e suficientes
para filtrar conjuntos intermediários de documentos. Estes conjuntos
filtrados são posteriormente processados por modelos densos, otimizando
assim o pipeline completo de recuperação de informações.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{Template_SBC/template-latex/images/cronograma.png}
    \caption{Cronograma de execução do projeto}
    \label{fig:cronograma}
\end{figure}

\vspace{1em}
\FloatBarrier


\section{Conclusão}
\label{sec:conclusao}
Este estudo investigou comparativamente a eficácia de diferentes algoritmos
de recuperação de informação, com particular ênfase na aplicação em sistemas
de Recuperação Aumentada por Geração (RAG). Os resultados experimentais
confirmaram as hipóteses iniciais e revelaram \textit{insights} significativos
sobre a arquitetura ótima para sistemas de recuperação contemporâneos.

\subsection{Principais Contribuições}

A pesquisa demonstrou que a implementação de algoritmos mais robustos,
baseados em \textit{embeddings} densos, resulta em melhorias substanciais
nas métricas de qualidade de recuperação. Especificamente, os modelos
\textit{Multilingual E5-Large-Instruct} e \textit{GTE-QWEN2} apresentaram
ganhos de 38-41\% no MRR@10 comparativamente ao \textit{baseline} BM25.
Estes resultados validam a superioridade dos métodos baseados em
representações semânticas densas.

Conforme previsto pela análise teórica, o algoritmo BM25 demonstrou
capacidade efetiva de criar conjuntos de dados miniaturizados que
servem como entrada otimizada para algoritmos de pontuação mais precisos.
Esta funcionalidade de \textit{data pruning} permite que modelos
computacionalmente intensivos operem sobre subconjuntos relevantes.
Dessa forma, maximiza-se a qualidade dos resultados sem comprometer
a escalabilidade do sistema.

\subsection{Validação dos \textit{Pipelines} Híbridos}

Os resultados experimentais forneceram evidência robusta de que algoritmos
híbridos constituem alternativas superiores para otimização de sistemas RAG.
O \textit{pipeline} híbrido de duas fases alcançou MRR@10 de 0.418,
representando melhoria de 46.7\% sobre o BM25 isolado. O \textit{pipeline}
de três fases atingiu 0.431, demonstrando refinamento adicional através
de estratificação algorítmica.

Esta arquitetura híbrida resolve efetivamente o dilema fundamental entre
cobertura documental e precisão semântica. Permite que sistemas RAG
operem sobre \textit{corpus} extensos mantendo alta qualidade de recuperação.
A abordagem representa uma solução equilibrada para os desafios
contemporâneos de recuperação de informação.

\subsection{Limitações das Implementações \textit{Standalone}}

O estudo revelou limitações críticas das implementações \textit{standalone}
de \textit{embeddings} densos, particularmente no que concerne ao custo
computacional. Os algoritmos de \textit{embedding} apresentaram latências
11-13 vezes superiores ao BM25 (142-157ms vs 12.3ms). Este resultado
implica em \textit{throughput} reduzido que inviabiliza aplicações em
tempo real sobre \textit{corpus} extensos.

Adicionalmente, as implementações \textit{standalone} falham em explorar
a complementaridade entre recuperação léxica e semântica. Isso resulta
em \textit{performance} subótima comparativamente aos \textit{pipelines}
híbridos. Este resultado sugere que abordagens puramente densas representam
uma otimização local que negligencia benefícios sistêmicos disponíveis
através de arquiteturas compostas.

\subsection{Recomendações Arquiteturais}

Com base nos resultados obtidos, recomenda-se a substituição de implementações
\textit{standalone} de \textit{embeddings} por \textit{pipelines} híbridos
em sistemas RAG de produção. As recomendações específicas incluem:
sistemas de alta demanda devem utilizar \textit{pipeline} de duas fases
(BM25 + \textit{embedding} denso) para equilíbrio ótimo entre
\textit{performance} e eficiência.

Para aplicações críticas, sugere-se \textit{pipeline} de três fases
para maximização da qualidade de recuperação. Em cenários de recursos
limitados, BM25 \textit{standalone} constitui solução de compromisso
aceitável. Estas diretrizes fornecem um \textit{framework} de decisão
baseado em evidências empíricas para implementações práticas.

\subsection{Impacto para Sistemas RAG}

Os resultados demonstram que a arquitetura híbrida proposta endereça
limitações fundamentais dos sistemas RAG contemporâneos em três dimensões
principais. A escalabilidade é assegurada através da utilização do BM25
como filtro inicial, permitindo operação sobre \textit{corpus} de magnitude
arbitrária sem degradação linear da \textit{performance}.

A qualidade é garantida pelo reranqueamento através de \textit{embeddings}
densos, assegurando que a recuperação final capture nuances semânticas
críticas para geração de respostas precisas. A eficiência é otimizada
pela estratificação computacional, aplicando algoritmos intensivos apenas
onde necessário. Esta arquitetura representa uma solução madura para
os desafios de sistemas RAG em escala industrial.

\subsection{Direções Futuras}

Os resultados obtidos sugerem várias direções promissoras para pesquisa
futura. A otimização adaptativa representa área de interesse, envolvendo
desenvolvimento de mecanismos que ajustem dinamicamente os parâmetros
do \textit{pipeline} baseado em características da \textit{query} e
do domínio. A integração multimodal constitui outra direção relevante.

A extensão da arquitetura híbrida para incorporar modalidades adicionais
(imagem, áudio) em sistemas RAG multimodais oferece potencial significativo.
A personalização contextual através de técnicas para personalização
dos \textit{embeddings} baseada no contexto do usuário representa área
de investigação promissora. A análise de eficiência energética dos
diferentes algoritmos constitui direção de pesquisa contemporaneamente
relevante.

\subsection{Considerações Finais}

Este trabalho contribui para o avanço do campo de recuperação de informação
demonstrando empiricamente a superioridade de arquiteturas híbridas sobre
implementações \textit{standalone}. Os resultados fornecem diretrizes
concretas para otimização de sistemas RAG, oferecendo fundamentação
científica para decisões arquiteturais em implementações práticas.

A validação experimental confirma que a evolução dos sistemas de recuperação
não reside na substituição completa de algoritmos tradicionais, mas na
orquestração inteligente de múltiplas técnicas complementares. Esta
abordagem híbrida representa um paradigma maduro que balanceia efetivamente
as demandas conflitantes de qualidade, eficiência e escalabilidade em
sistemas de inteligência artificial contemporâneos.

O impacto destes achados estende-se além do domínio técnico, influenciando
decisões estratégicas sobre infraestrutura computacional e alocação de
recursos em organizações que implementam sistemas RAG em escala industrial.
A metodologia proposta oferece um \textit{framework} replicável para
avaliação e otimização de sistemas de recuperação. Contribui assim para
a padronização de práticas no campo.

\bibliographystyle{sbc}
\bibliography{sbc-template}

\end{document}
